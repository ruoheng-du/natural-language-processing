{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOVigQAS+vHcbaYXBz4tntO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3plKGZ7_Asde","executionInfo":{"status":"ok","timestamp":1697888034339,"user_tz":-480,"elapsed":23061,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}},"outputId":"b4feb694-9fcb-4c25-8917-76df1f2d7f7c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dF__u69PdawH","executionInfo":{"status":"ok","timestamp":1697888139958,"user_tz":-480,"elapsed":25450,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}},"outputId":"8d7c6665-057d-4ea8-e849-a3e5aec4cd25"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd /content/drive/MyDrive/text_classification/dot/word2vec-ernie"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sioJcyFqddhQ","executionInfo":{"status":"ok","timestamp":1697888155794,"user_tz":-480,"elapsed":433,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}},"outputId":"dd2a882e-f9c3-459a-a420-d2ee29656095"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/text_classification/dot/word2vec-ernie\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","import jieba"],"metadata":{"id":"3Kl726qmdqwc","executionInfo":{"status":"ok","timestamp":1697888168807,"user_tz":-480,"elapsed":10048,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["## ernie ##\n","# 读取数据\n","data = pd.read_excel('./data.xlsx')\n","sentences = data['review'].tolist()\n","\n","def txt_cut(s):\n","    res = [w for w in jieba.lcut(s) if w.strip()]\n","    return \" \".join(res)\n","\n","# 对文本进行分词\n","tokenized_text = []\n","for i in sentences:\n","    new = txt_cut(i).split(' ')\n","    tokenized_text.extend(new)\n","\n","# 初始化ERNIE tokenizer 和 model\n","tokenizer = AutoTokenizer.from_pretrained('nghuyong/ernie-1.0')\n","model = AutoModel.from_pretrained('nghuyong/ernie-1.0')\n","\n","# 用于存储单词和词向量的字典\n","word_vectors_dict = {}\n","\n","# 使用jieba分词并处理每个单词\n","for word in tokenized_text:\n","    # 使用ERNIE tokenizer 对单词进行编码\n","    inputs = tokenizer(word, return_tensors='pt', truncation=True, padding=True)\n","    input_ids = inputs['input_ids']\n","    attention_mask = inputs['attention_mask']\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","\n","    # 获取单词的词向量\n","    word_vector = outputs.last_hidden_state.squeeze(0).numpy()\n","\n","    # 存储单词和对应词向量\n","    word_vectors_dict[word] = word_vector"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AkWhSjKcIEVj","executionInfo":{"status":"ok","timestamp":1697890948540,"user_tz":-480,"elapsed":10010,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}},"outputId":"94755766-f08a-419a-9a6a-700b2a7343c4"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from gensim.models import Word2Vec\n","import jieba"],"metadata":{"id":"6YkkmRhzA4tB","executionInfo":{"status":"ok","timestamp":1697888509786,"user_tz":-480,"elapsed":2055,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["## word2vec ##\n","# 初始化Word2Vec模型\n","model_w = Word2Vec(vector_size=100, window=5, min_count=1)\n","\n","# 构建词汇表\n","model_w.build_vocab([tokenized_text])\n","\n","# 训练模型\n","model_w.train([tokenized_text], total_examples=model_w.corpus_count, epochs=model_w.epochs)\n","\n","# 创建一个空的DataFrame来保存词汇和对应的词向量\n","word_vector_df = pd.DataFrame()\n","\n","for wd in model_w.wv.index_to_key:\n","    # 将词向量转化为Series，然后添加到DataFrame中\n","    word_vector_series = pd.Series([wd] + list(model_w.wv[wd]))\n","    word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YIiaDKizA7sY","executionInfo":{"status":"ok","timestamp":1697890956568,"user_tz":-480,"elapsed":463,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}},"outputId":"25e97172-45ce-4f1d-dc00-b61de8e16748"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n","<ipython-input-52-cc1d6ea3e34b>:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n","  word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)\n"]}]},{"cell_type":"code","source":["new_word_vectors = []\n","\n","## ernie\n","for word, vector in word_vectors_dict.items():\n","    word_vector_r = vector[0]\n","    word_vector_r = word_vector_r.reshape(-1, 1)\n","\n","    ## word2vec\n","    word_vector_w = model_w.wv[word]\n","    word_vector_w = word_vector_w.reshape(-1, 1)\n","\n","    ## 新词向量\n","    new_word_vector = np.multiply(word_vector_r, word_vector_w.T)\n","    new_word_vectors.append((word, new_word_vector))"],"metadata":{"id":"AyswwpRUA_Au","executionInfo":{"status":"ok","timestamp":1697891018169,"user_tz":-480,"elapsed":588,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# 指定保存文件路径\n","save_path = 'new_word_vectors.txt'\n","\n","# 保存单词和新词向量到文件\n","with open(save_path, 'w') as f:\n","    for word, word_vector in new_word_vectors:\n","        word_vector_str = ','.join(map(str, word_vector))\n","        line = f\"{word}: {word_vector_str}\\n\"\n","        f.write(line)"],"metadata":{"id":"rkv_uAAxBA_L","executionInfo":{"status":"ok","timestamp":1697891091933,"user_tz":-480,"elapsed":24030,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}}},"execution_count":58,"outputs":[]}]}