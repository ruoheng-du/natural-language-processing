{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPXmrXsJ+TrMBw08+A4w1+Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kc9Kvwa_SmoU","executionInfo":{"status":"ok","timestamp":1697876047331,"user_tz":-480,"elapsed":34031,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}},"outputId":"57d1c287-8ccc-4f4a-9c9f-15751ed17571"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vby94mLZSUeR"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","from transformers import DebertaTokenizer, DebertaModel"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TB3BhIBSSegO","executionInfo":{"status":"ok","timestamp":1697876090126,"user_tz":-480,"elapsed":21834,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}},"outputId":"3a075241-bb62-411c-cc5d-58296cdf1ca4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/text_classification/dot/word2vec-deberta"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5MgRkwFSe9F","executionInfo":{"status":"ok","timestamp":1697876091804,"user_tz":-480,"elapsed":310,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}},"outputId":"193a6e59-cff7-4904-d44b-1ee82bafd792"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/text_classification/dot/word2vec-deberta\n"]}]},{"cell_type":"code","source":["## deberta ##\n","# 读取数据\n","data = pd.read_excel('./data.xlsx')\n","sentences = data['review'].tolist()\n","\n","tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n","model = DebertaModel.from_pretrained('microsoft/deberta-base')\n","\n","word_vectors = []\n","\n","# 遍历每个句子并获取单词的词向量\n","for sentence in sentences:\n","    # 使用 tokenizer 对句子进行分词\n","    tokens = tokenizer.tokenize(sentence)\n","\n","    # 遍历分词后的 tokens 并获取每个单词的词向量\n","    for token in tokens:\n","        # 将单词 token 传递给模型以获取词向量\n","        inputs = tokenizer(token, return_tensors='pt', padding=True, truncation=True)\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            word_vector = outputs.last_hidden_state.squeeze(0).mean(dim=0).cpu().numpy()  # 获取词向量并取平均\n","            word_vectors.append((token, word_vector))"],"metadata":{"id":"SoCDeJhbSXGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 指定的单词\n","# word = '宗教'\n","\n","# # 使用 tokenizer 对单词进行编码\n","# inputs = tokenizer(word, return_tensors='pt', padding=True, truncation=True)\n","\n","# # 传递编码后的输入到 DeBERTa 模型以获取词向量\n","# with torch.no_grad():\n","#     outputs = model(**inputs)\n","\n","# # 获取单词 \"宗教\" 的词向量\n","# word_vector = outputs.last_hidden_state.squeeze(0).mean(dim=0).cpu().numpy()\n","\n","# # 打印单词 \"宗教\" 的词向量\n","# print(f\"Word: {word}, Word Vector: {word_vector}\")"],"metadata":{"id":"KvyIwENcZmOD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from gensim.models import Word2Vec\n","import jieba"],"metadata":{"id":"ubu1XYxLrkRJ","executionInfo":{"status":"ok","timestamp":1697878493489,"user_tz":-480,"elapsed":1466,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["## word2vec ##\n","def txt_cut(s):\n","    res = [w for w in jieba.lcut(s) if w.strip()]\n","    return \" \".join(res)\n","\n","# 对文本进行分词\n","tokenized_text = []\n","for i in sentences:\n","    new = txt_cut(i).split(' ')\n","    tokenized_text.extend(new)\n","\n","# 初始化Word2Vec模型\n","model_w = Word2Vec(vector_size=100, window=5, min_count=1)\n","\n","# 构建词汇表\n","model_w.build_vocab([tokenized_text])\n","\n","# 训练模型\n","model_w.train([tokenized_text], total_examples=model_w.corpus_count, epochs=model_w.epochs)\n","\n","# 创建一个空的DataFrame来保存词汇和对应的词向量\n","word_vector_df = pd.DataFrame()\n","\n","for wd in model_w.wv.index_to_key:\n","    # 将词向量转化为Series，然后添加到DataFrame中\n","    word_vector_series = pd.Series([wd] + list(model_w.wv[wd]))\n","    word_vector_df = word_vector_df.append(word_vector_series, ignore_index=True)"],"metadata":{"id":"QVA_ZxgWqmDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# word = \"宗教\"\n","# word_vector_2 = model_w.wv[word]\n","# print(word_vector_2)"],"metadata":{"id":"23iT0CJLuH2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["new_word_vectors = []\n","\n","for word in tokenized_text:\n","    ## deberta\n","    # 使用 tokenizer 对单词进行编码\n","    inputs = tokenizer(word, return_tensors='pt', padding=True, truncation=True)\n","\n","    # 传递编码后的输入到 DeBERTa 模型以获取词向量\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # 获取单词 \"宗教\" 的词向量\n","    word_vector_d = outputs.last_hidden_state.squeeze(0).mean(dim=0).cpu().numpy()\n","\n","    word_vector_d = word_vector_d.reshape(-1, 1)\n","\n","    ## word2vec\n","    word_vector_w = model_w.wv[word]\n","\n","    word_vector_w = word_vector_w.reshape(-1, 1)\n","\n","    ## 新词向量\n","    new_word_vector = np.multiply(word_vector_d, word_vector_w.T)\n","\n","    new_word_vectors.append((word, new_word_vector))"],"metadata":{"id":"X5EY0ecRcxzx","executionInfo":{"status":"ok","timestamp":1697879187606,"user_tz":-480,"elapsed":11468,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["# 指定保存文件路径\n","save_path = 'new_word_vectors.txt'\n","\n","# 保存单词和新词向量到文件\n","with open(save_path, 'w') as f:\n","    for word, word_vector in new_word_vectors:\n","        word_vector_str = ','.join(map(str, word_vector))\n","        line = f\"{word}: {word_vector_str}\\n\"\n","        f.write(line)"],"metadata":{"id":"CS28Rp-4o4_T","executionInfo":{"status":"ok","timestamp":1697879703973,"user_tz":-480,"elapsed":33014,"user":{"displayName":"Ruoheng Du","userId":"08748997774430241707"}}},"execution_count":61,"outputs":[]}]}