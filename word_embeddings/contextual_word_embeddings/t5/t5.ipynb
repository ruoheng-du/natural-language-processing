{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1-jIfLSCNnr",
        "outputId": "6e25f823-0352-4920-fbfc-53065f923f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz2OutEJLOPN",
        "outputId": "23e773f2-dcea-49dc-a1a9-6c887bcf15ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5Model\n",
        "import torch"
      ],
      "metadata": {
        "id": "XglbM4qgCWrc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpa4n58VCcPe",
        "outputId": "8a5a2bae-8315-47c5-831e-8c6ae1e0289d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/text_classification/word_vectors/t5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wROqqyOeCk3D",
        "outputId": "c812bfa7-5fe5-4611-d131-bb239ea4ea3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/text_classification/word_vectors/t5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read excel file\n",
        "df = pd.read_excel('./data.xlsx')\n",
        "\n",
        "# Initialize T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "model = T5Model.from_pretrained('t5-base')\n",
        "\n",
        "# Create a list to store all the word vectors\n",
        "all_vectors = []\n",
        "\n",
        "# For each row in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    # Input text\n",
        "    text = row['review']\n",
        "\n",
        "    # Encode the input\n",
        "    inputs = tokenizer.encode_plus(text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # Provide dummy decoder inputs\n",
        "    inputs['decoder_input_ids'] = torch.tensor([[0]])\n",
        "\n",
        "    # Pass the input through the model and get the word vectors\n",
        "    outputs = model(**inputs)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "    # Append the tensor to all_vectors\n",
        "    all_vectors.append(last_hidden_states)\n",
        "\n",
        "# Concatenate the tensors along the batch dimension\n",
        "all_vectors = torch.cat(all_vectors, dim=0)\n",
        "\n",
        "# Save to file\n",
        "torch.save(all_vectors, 't5_vectors.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg1UsFeCM1F_",
        "outputId": "ea8238ff-78eb-4270-f579-c054c82b5394"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:220: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get a word vector\n",
        "word = \"宗教\"\n",
        "word_index = tokenizer.convert_tokens_to_ids(word)\n",
        "word_vector = all_vectors[word_index]"
      ],
      "metadata": {
        "id": "VgbhIsXkQSpu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUhaxowkRTSL",
        "outputId": "a2ad9551-ee87-4fbc-86f2-1041ecaf5efb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-8.1013e-02,  3.0991e-03,  7.0416e-02, -1.1984e-01,  1.0911e-02,\n",
              "          5.4737e-02,  9.7534e-04, -4.6635e-02, -3.0373e-02, -1.9431e-02,\n",
              "         -3.1763e-02, -2.3609e-02, -9.9756e-01, -4.2645e-02,  4.5994e-02,\n",
              "          4.0119e-02, -1.3130e-02, -9.3535e-02,  9.0953e-02,  9.6026e-03,\n",
              "          4.0389e-02, -6.0000e-02, -6.8650e-02,  3.6024e-02, -3.9379e-03,\n",
              "          5.9535e-02, -1.0701e-01,  8.1566e-01, -2.0432e-06, -7.9362e-03,\n",
              "          3.3575e-02,  1.2464e+00, -1.1482e-03,  4.0915e-03, -1.4089e-01,\n",
              "          2.3354e-02,  5.1622e-02, -1.0110e-01, -8.8161e-02, -6.5211e-04,\n",
              "          4.4628e-02,  4.1752e-02,  6.0915e-02,  3.9070e-02,  2.3365e-01,\n",
              "          7.8357e-02, -2.0556e-02, -9.3222e-02, -1.8359e-02, -5.4452e-02,\n",
              "         -4.0602e-02, -8.1542e-02, -1.7528e-01,  5.6859e-02,  5.8969e+00,\n",
              "         -1.7546e-02, -9.6429e-02,  1.4797e-01, -3.5602e-03,  7.4621e-02,\n",
              "          9.4105e-02,  5.7047e-02,  6.7089e-02,  1.8988e-02,  7.2183e-02,\n",
              "          6.8254e-01,  3.5832e-02, -4.8175e-02, -9.0429e-01,  6.0762e-02,\n",
              "          3.1422e-02, -1.5585e-02,  9.3563e-02,  4.1091e-02, -2.7048e-02,\n",
              "          5.6177e-02, -4.7990e-02, -8.6991e-03,  4.0092e-02,  3.9170e-02,\n",
              "          7.7349e-02, -4.1420e-02,  5.3974e-02,  5.2044e-02,  5.2471e-02,\n",
              "          1.2816e-02,  6.1219e-01, -4.2352e-02, -4.0219e-02, -2.2216e-02,\n",
              "          1.2848e-01, -6.0311e-02,  7.8751e-02, -7.1087e-02, -6.1426e-02,\n",
              "          4.1191e-02, -3.8828e-02,  5.1431e-02,  3.7706e-02,  2.3095e-03,\n",
              "         -4.0921e-02, -1.2918e-03,  2.6422e-02, -4.4106e-03, -6.8369e-02,\n",
              "         -1.1479e-01,  3.2206e-03,  6.7129e-02,  6.0397e-02,  2.9249e-02,\n",
              "          1.5445e-02, -1.9828e-02, -2.2614e-02,  3.9668e-02,  3.4557e-03,\n",
              "         -1.7809e-03,  1.7949e-01, -1.5529e-01,  3.4459e-02, -4.7761e-02,\n",
              "          1.7114e-02,  1.0225e-01, -2.7011e-01,  4.4318e-03, -9.3671e-02,\n",
              "         -3.1205e-02,  3.9210e-02, -4.5190e-02, -6.6668e-02, -1.8555e-01,\n",
              "          3.4743e-01, -6.4056e-02, -7.0030e-02, -1.6810e-02,  2.0300e-02,\n",
              "         -4.1936e-02, -8.0180e-03, -1.1495e-01, -2.7494e-02,  8.5186e-02,\n",
              "         -5.9074e-02, -2.6330e-02, -4.0900e-02, -2.1514e-01,  1.1555e-02,\n",
              "         -5.9573e-03,  5.4876e-02,  1.3092e-01, -1.0780e-02, -1.0677e-01,\n",
              "         -1.3681e-01,  6.1762e-02,  2.1900e-03, -1.9571e-01,  5.2109e-02,\n",
              "          2.0282e-02,  4.7804e-02,  3.5366e-02, -6.1950e-01, -1.0474e-01,\n",
              "         -2.2438e-02, -7.9190e-02, -1.6126e-02, -6.0101e-02,  3.9686e-02,\n",
              "         -6.9167e-03,  1.6204e-02,  4.2108e-02, -1.9113e-01, -1.3129e-02,\n",
              "          1.3882e-01,  7.9524e-02,  7.7892e-02,  1.2478e-01,  1.4927e-02,\n",
              "          2.4276e-02, -6.6489e-03, -5.2718e-02, -1.8340e-01,  9.2559e-02,\n",
              "          7.7353e-02, -5.1401e-02, -1.3816e-02, -5.5846e-02, -2.1428e-01,\n",
              "          1.0022e-01, -3.1488e-02, -6.4165e-01, -5.8983e-02,  8.5073e-05,\n",
              "         -3.6632e-02,  2.3488e-02,  3.7076e-02,  5.9644e-02,  1.1817e-02,\n",
              "         -6.3665e-02,  1.3497e-02, -1.7807e-02, -4.7077e-02, -1.9975e-02,\n",
              "          5.2394e-03, -1.3033e-02,  7.4497e-02,  8.7927e-02,  8.8553e-03,\n",
              "         -7.4334e-02,  2.4279e-02,  4.9173e-02, -9.8213e-02, -6.1261e-02,\n",
              "          4.2244e-02,  5.7641e-03, -1.7907e-02, -3.6754e-02, -3.0526e-02,\n",
              "         -1.3676e-02,  1.2308e-02, -3.0781e-02, -1.1272e-01,  8.2415e+00,\n",
              "         -1.8022e-02, -5.1069e-03, -4.6942e-02, -5.5763e-02, -1.8150e-02,\n",
              "          2.9472e-02,  9.2516e-03, -1.2116e-01,  7.2598e-02, -1.0525e-01,\n",
              "          1.0192e-01,  1.6251e-02,  2.0352e-04,  5.1253e-02,  4.3927e-02,\n",
              "          2.4578e-02, -1.9788e-02, -3.2819e-02, -1.8947e-02, -2.3892e-02,\n",
              "         -5.6693e+00, -4.8951e-02, -8.1201e-02,  7.8173e-03, -5.0817e-02,\n",
              "         -1.6712e-01, -4.4303e-02, -2.5184e-03, -4.6062e-02, -1.6587e-02,\n",
              "         -1.9667e-02, -4.8634e-02,  9.6620e-03, -8.8335e-02,  5.2961e-02,\n",
              "          6.4679e-02, -3.9028e-02,  5.5938e-02,  1.0343e-02,  5.9680e-02,\n",
              "         -1.9609e-01, -4.9054e-02,  4.9768e-02, -5.1154e-02, -1.0451e-02,\n",
              "          3.0629e-02, -6.8592e-02,  9.0706e-02,  4.2310e-02, -4.1375e-02,\n",
              "          8.7910e-02, -9.8411e-01,  1.5350e-02, -1.4097e-01,  5.8411e-01,\n",
              "          1.3232e-01, -1.5049e-02,  7.5708e-02, -9.0558e-02,  3.6750e-02,\n",
              "         -8.4895e-02, -8.0958e-02, -1.4510e-01,  1.4642e-02,  4.0039e-02,\n",
              "          1.9126e-02,  2.5448e-02, -5.7622e-02,  5.6747e-02, -4.3333e-02,\n",
              "          2.7087e-02,  2.3585e-03,  6.4187e-03,  4.5284e-02,  8.4375e-03,\n",
              "          2.9989e-02,  7.0834e-01, -7.5006e-02,  3.1816e-05, -1.0348e-01,\n",
              "         -8.2393e-02,  4.9905e-02, -4.1994e-02, -2.9709e-02, -4.6893e-02,\n",
              "          7.9171e-02, -6.2389e-03, -3.0287e-03, -1.1808e-01, -9.6132e-03,\n",
              "         -1.1614e-02,  1.1926e-01, -7.4112e-02,  5.5145e-02, -1.7295e-01,\n",
              "         -4.2119e-02,  7.7891e-02,  2.4878e-01, -9.1072e-02, -5.6336e-02,\n",
              "          4.1416e-02,  4.3497e-02, -2.6310e-02, -7.8621e-03,  4.3824e-02,\n",
              "          3.0441e-02,  1.1611e-01,  4.7742e-02, -1.6341e-01,  4.2090e-02,\n",
              "          1.8206e-02,  1.3749e-01, -8.3267e-02,  1.5961e-02, -6.4699e-02,\n",
              "          4.7744e-02,  2.3722e-02, -1.9880e-02, -7.8964e-02, -9.5012e-01,\n",
              "          3.1981e-03, -3.0123e-02, -3.4650e-02, -1.7656e-03, -1.1681e-02,\n",
              "         -5.2088e-02, -3.6344e-02, -5.2224e-02,  3.5220e-01,  5.2826e-02,\n",
              "          8.7437e-02, -2.1902e-02,  1.5842e-02, -2.4598e-02, -1.1911e-01,\n",
              "         -1.2093e-01, -5.5921e-02,  4.9210e-02,  1.9130e-02,  8.0875e-02,\n",
              "          5.2091e-02,  1.6609e-02, -1.2156e-02,  1.8712e-01,  2.4346e-02,\n",
              "         -4.9469e-02,  1.2019e-03,  3.2484e-02, -6.2010e-02,  1.6213e-02,\n",
              "          5.9789e-02, -2.1247e-02, -1.7761e-01,  6.3755e-02, -9.4757e-02,\n",
              "         -4.5589e-02,  4.6814e-02,  1.1222e-01,  2.9328e-02,  6.7860e-02,\n",
              "         -3.8865e-02, -1.1567e-01,  1.0309e+00,  2.6608e-02,  8.9150e-02,\n",
              "         -1.5358e-01, -1.1018e-01, -1.6925e-01, -2.8368e-02,  6.5259e-03,\n",
              "         -2.9702e-02,  8.1172e-02, -1.4071e-02,  1.5842e-01, -4.7547e-02,\n",
              "         -5.5391e-02,  6.3202e-02, -7.0221e-02, -5.0082e-02, -8.0436e-02,\n",
              "         -6.3408e-03,  4.4037e-02, -7.5696e-02,  3.6675e-03, -1.7135e-02,\n",
              "          1.3314e-01,  8.1979e-02,  2.4599e-02, -6.8400e-03, -5.9330e-02,\n",
              "          2.7677e-02,  5.2213e-02,  3.8971e-02, -1.1397e-01, -4.7292e-02,\n",
              "          1.0878e-01, -7.8840e-02, -2.6986e-02, -8.6513e-02, -2.7881e-02,\n",
              "         -7.3593e+00, -1.4848e-02,  2.7503e-02,  3.4424e-01, -3.0483e-03,\n",
              "          1.4300e-02,  9.9354e-02,  5.7230e-02,  3.1325e-02, -9.0514e-03,\n",
              "          1.3661e-02,  1.0998e-02, -1.1700e-02, -6.0351e-02,  5.6189e-02,\n",
              "          1.2074e-01, -2.3113e-02,  1.8812e-02, -4.7955e-02, -1.3857e-01,\n",
              "          6.6045e-02,  3.8565e-02, -7.4918e-03, -1.6998e-01,  2.5771e-02,\n",
              "         -7.2400e-02,  6.8573e-03, -2.4832e-02, -3.0717e-01, -1.4734e-02,\n",
              "         -2.6932e-02, -1.1617e-01,  4.6183e-02,  6.5440e-02, -2.9260e-02,\n",
              "         -5.0986e-02, -6.2121e-02, -8.4534e-02,  3.3127e-02,  1.8033e-02,\n",
              "          1.1143e-02, -8.4111e-02, -3.3351e-03, -4.7551e-02, -1.1150e-01,\n",
              "          3.4959e-02,  2.5791e-02,  4.2502e-03,  4.1372e-02, -8.8285e-03,\n",
              "          7.0752e-03,  9.8343e-02, -1.9205e-01, -6.1863e-02, -1.0990e-02,\n",
              "          7.3739e-02, -8.9169e-02, -4.9972e-02,  8.5917e-02,  8.4337e-02,\n",
              "         -4.1881e-02, -1.0224e-01,  1.1561e-02, -1.6285e-02,  1.0020e+00,\n",
              "          1.9333e-02,  1.6473e-02,  1.5147e+00, -7.5029e-02, -1.1285e-01,\n",
              "          3.2790e-02, -1.7134e-02, -8.4248e-02, -6.3487e-02,  1.3350e-02,\n",
              "         -6.4377e+00, -5.5052e-03, -2.3298e-02, -2.1608e-02,  3.5283e-02,\n",
              "          5.0238e-02,  9.7451e-02,  2.1607e-02, -4.3470e-02, -4.8839e-02,\n",
              "         -1.2186e-02, -8.7274e-02, -2.7113e-02,  4.5823e-02,  3.8472e-02,\n",
              "          3.5523e-02, -1.6995e-02, -6.2438e-02,  2.6831e-02, -1.1897e-02,\n",
              "         -1.2956e-02, -3.8556e-02,  2.8511e-02, -7.9524e-01, -1.5423e-02,\n",
              "          9.1535e-02, -1.1106e-01,  4.7916e-02,  1.7646e-01,  2.5058e-02,\n",
              "         -5.9627e-02, -1.1962e-01,  8.2558e-01, -1.2847e-02, -1.6079e-02,\n",
              "         -1.8015e-01,  5.1089e-02,  1.3709e-02,  6.9480e-02,  1.4516e-01,\n",
              "          1.1194e-01, -1.0388e-01,  7.1244e-03,  6.1012e-02,  5.5900e-02,\n",
              "         -1.0636e-01,  9.3107e-03, -1.0769e-01,  5.3854e-02, -1.0700e-02,\n",
              "         -4.1103e-02, -7.1492e-02,  4.9010e-02,  1.8794e-02,  4.9502e-02,\n",
              "          3.5082e-02, -1.4829e-01,  5.6575e-02,  1.3444e+00,  5.0013e-02,\n",
              "          4.8017e-02, -4.4435e-02, -7.8658e-02, -5.9784e-02, -1.4242e-01,\n",
              "         -1.1061e-02,  2.3355e-02, -5.1954e-02, -4.8127e+00, -9.3731e-02,\n",
              "         -1.9179e-01,  1.0010e-01,  2.3192e-02, -4.7310e-02, -1.9721e-02,\n",
              "         -5.3621e-02,  7.1562e-02, -4.1463e-02, -5.6751e-02,  1.5150e-01,\n",
              "          4.9505e-02, -4.4430e-03,  5.3347e-02,  4.1753e-02,  6.8012e-02,\n",
              "          1.0541e-01, -6.9755e-02, -2.4459e-02,  1.1011e-01, -3.5651e-02,\n",
              "          3.6796e-02,  4.9417e-02, -3.0345e-02, -1.1169e-02, -3.7075e-02,\n",
              "          1.5232e-02,  5.1719e-02, -3.3797e-02,  1.7721e-01, -7.1735e-02,\n",
              "          3.0935e-02,  2.1266e-02,  4.2520e-02,  1.3122e+00,  2.2907e-02,\n",
              "         -1.6315e-01, -3.6363e-02, -1.0670e-01, -4.5648e-02,  1.2356e-01,\n",
              "          8.0046e-02, -4.1240e-02, -3.1423e-01, -1.0934e-02,  3.9904e-02,\n",
              "         -3.7748e-02,  4.0598e-02, -1.5332e-02, -8.0819e-02, -2.8370e-02,\n",
              "          3.7911e-02, -2.1385e-02, -1.9626e-01, -2.7218e-02, -4.5127e-02,\n",
              "         -1.6264e-02,  4.6745e-02, -1.6085e-02, -1.1557e-01,  2.2780e-02,\n",
              "         -2.2193e-02,  2.3025e-02, -5.3906e-02,  4.7526e-02, -2.5526e-02,\n",
              "          4.4501e-02, -6.1716e-02, -1.3848e-01, -1.2294e+00,  7.8194e-02,\n",
              "          4.5510e-02,  5.5434e-02,  4.7551e-02, -5.2453e-02,  5.2404e-02,\n",
              "         -6.4723e-02,  2.7988e-02, -7.1350e-02,  1.5159e-02,  4.8664e-01,\n",
              "         -1.7341e-02, -1.6652e-01, -1.0570e-02, -3.6478e-02, -1.6047e-02,\n",
              "         -5.3357e-03, -2.8169e-02, -1.0078e-01, -1.1193e-02, -9.9396e-03,\n",
              "          2.8870e-01, -1.6937e-03,  1.2593e-03,  1.0419e-01,  6.5106e-02,\n",
              "          2.7937e-02, -4.0680e-02,  4.8401e-02, -2.3793e-01,  4.0926e-02,\n",
              "         -3.1861e-02, -5.9248e-02, -1.0988e-01,  4.5334e-02,  6.1851e-02,\n",
              "          3.8321e-02, -1.6157e-02, -6.5078e-02,  4.3460e-02,  1.3135e-01,\n",
              "          1.3622e-02,  4.4923e-02,  9.3573e-02, -3.5223e-03, -1.2527e-02,\n",
              "          6.7116e-02, -4.0944e-02,  6.6757e-02,  1.7307e-02,  6.1792e-02,\n",
              "         -3.9096e-02,  7.0870e-02,  1.0911e-01,  8.3093e-02,  1.0078e-02,\n",
              "         -3.3632e-02, -6.0565e-02, -3.0215e-02,  2.8494e-02,  4.1957e-02,\n",
              "         -2.2052e-02, -1.9979e-02, -4.7943e-02, -1.1188e-01, -3.8483e-02,\n",
              "          9.8060e-02, -1.2484e-01,  4.7646e-02,  7.0407e-03,  1.3885e-01,\n",
              "          9.7039e-02,  4.7926e-02,  2.1186e-02, -3.3180e-01, -2.0002e-02,\n",
              "          7.9331e-02, -3.6277e-02,  9.4674e-03, -4.3626e-02,  1.4162e-02,\n",
              "          6.2713e-02, -8.9368e-02, -7.0768e-02,  9.1982e-03,  6.0324e-02,\n",
              "          4.3371e-02,  2.2858e-02, -3.0322e-01,  3.9354e-03, -1.2331e-01,\n",
              "         -1.5188e-02, -8.5849e-02,  1.5932e-01,  1.9050e-03,  6.2806e-02,\n",
              "          7.7223e-02,  4.5139e-02, -3.2433e-02,  8.4297e-01, -4.8155e-02,\n",
              "          7.8707e-03, -1.7146e-02, -6.9016e-02,  9.7655e-02,  9.0824e-02,\n",
              "         -5.3037e-02, -2.6119e-03,  1.8183e-02,  2.8020e-01, -2.9687e-02,\n",
              "          1.7120e-01,  1.2536e-01, -4.2463e-02, -1.8756e-01, -2.4582e-02,\n",
              "         -1.5067e-02, -3.6636e-02,  1.1706e-01,  7.0092e-03,  1.1122e-01,\n",
              "          1.6896e-02,  3.0596e-02,  1.9672e-02, -2.0090e-01, -2.6621e-03,\n",
              "         -9.6484e-02,  2.3772e-02, -1.1813e-01,  2.3507e-02,  3.6482e-02,\n",
              "          4.8038e-02, -2.1233e-02, -1.9002e-02]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector.size(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVfO4Ep0RXo9",
        "outputId": "9bc36420-a4bb-4af3-bd5e-b984f67d4d7c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}